{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfMJ1RH1qX6gcL238nsSkK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/auroraji2906/Ego4D_EpisodicMemory_Group19/blob/main/Some_qualitative_results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qualitative results\n",
        "\n",
        "Here there is no need to use GPU, you can execute all within CPU\n",
        "\n",
        "In this notebook we analyse some interesting qualitative results obtained using LLaVA and the following metrics: BLEU, ROUGE L, METEOR. For each of them we provide also the corresponding video clip used to answer the query."
      ],
      "metadata": {
        "id": "2rvmYnWYgH82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mount Google Drive:"
      ],
      "metadata": {
        "id": "64rYG6oxl8Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fauLJoiFviBW",
        "outputId": "36b16e3f-3f7e-4da9-c2f9-87ab1d5359c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBsjg8pN0knX"
      },
      "source": [
        "## Download Data and Setup Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcNVEU5Z8f5p"
      },
      "source": [
        "### **Fill In Your Access Info Here**\n",
        "If you don't have access and secret keys, first sign the Ego4D License at [ego4ddataset.com](https://ego4ddataset.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSvhBsBvnXy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIATEEVKTGZMNKNYPXA\"\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcIg7gNx82Bq"
      },
      "source": [
        "### **Set up CLIs and Download Annotations + Repo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-D9Jm-l162m",
        "outputId": "52095fb8-024b-4a87-be91-a1c7eb3a0ec0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 59.2M  100 59.2M    0     0   172M      0 --:--:-- --:--:-- --:--:--  172M\n"
          ]
        }
      ],
      "source": [
        "# Download the AWS and Ego4D CLIs, then download the annotations locally\n",
        "%%bash\n",
        "export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n",
        "export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n",
        "\n",
        "# Set up the AWS CLI\n",
        "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "unzip -o awscliv2.zip >/dev/null\n",
        "sudo ./aws/install >/dev/null 2>&1\n",
        "aws configure set aws_access_key_id \"$AWS_ACCESS_KEY_ID\" && aws configure set aws_secret_access_key \"$AWS_SECRET_ACCESS_KEY\"\n",
        "rm \"awscliv2.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tXEDSW50Ebd"
      },
      "source": [
        "### Install the ego4d CLI and Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg6Xt1p-On-a",
        "outputId": "84c64578-bc6b-49ae-8479-53bde111734f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ego4d\n",
            "  Downloading ego4d-1.7.3.tar.gz (94 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3 (from ego4d)\n",
            "  Downloading boto3-1.40.20-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ego4d) (4.67.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from ego4d) (2024.11.6)\n",
            "Collecting dataclasses_json (from ego4d)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting iopath (from ego4d)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.41.0,>=1.40.20 (from boto3->ego4d)\n",
            "  Downloading botocore-1.40.20-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->ego4d)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->ego4d)\n",
            "  Downloading s3transfer-0.13.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json->ego4d)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json->ego4d)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath->ego4d) (4.15.0)\n",
            "Collecting portalocker (from iopath->ego4d)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.20->boto3->ego4d) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.20->boto3->ego4d) (2.5.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json->ego4d) (25.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json->ego4d)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.20->boto3->ego4d) (1.17.0)\n",
            "Downloading boto3-1.40.20-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading botocore-1.40.20-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: ego4d, iopath\n",
            "  Building wheel for ego4d (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ego4d: filename=ego4d-1.7.3-py3-none-any.whl size=118282 sha256=f73f41ba0254ea45d164f7ed2ad28fed10ec5919065cb204dffd9e1819f80068\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/cb/71/5c67fe56e187aeb2d7566f197cd6987da12f7b718fd13a24be\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=592a640a49d51602392db8683ce75e97e2c3db0de334c2306f7a5b55ef756ab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built ego4d iopath\n",
            "Installing collected packages: portalocker, mypy-extensions, marshmallow, jmespath, typing-inspect, iopath, botocore, s3transfer, dataclasses_json, boto3, ego4d\n",
            "Successfully installed boto3-1.40.20 botocore-1.40.20 dataclasses_json-0.6.7 ego4d-1.7.3 iopath-0.1.10 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 portalocker-3.2.0 s3transfer-0.13.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Set up the Ego4D CLI\n",
        "!pip install ego4d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r \"/content/drive/MyDrive/NLQ\" \"/content/NLQ\""
      ],
      "metadata": {
        "id": "MrR0ygq0vgFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E0VTiQIu5Ox"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the annotated predictions from the JSON file\n",
        "with open(\"/content/NLQ/EXTENSION2/top50_annotated.json\", \"r\") as f:\n",
        "    top50 = json.load(f)\n",
        "\n",
        "# Extract all unique video_uids\n",
        "unique_video_uids = sorted(set(entry[\"video_uid\"] for entry in top50))\n",
        "\n",
        "# Save them into a temporary file to download the videos later\n",
        "with open(\"video_uid_list.txt\", \"w\") as f:\n",
        "    for uid in unique_video_uids:\n",
        "        f.write(uid + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGgkyBXUu_1X",
        "outputId": "39828e2c-472d-4a10-aca7-99f8bbfb37d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets to download: {'full_scale'}\n",
            "Download Path: /content/ego4d_data/v1\n",
            "Downloading Ego4D metadata json..\n",
            "Ego4D Metadata: /content/ego4d_data/ego4d.json\n",
            "Checking requested datasets and versions...\n",
            "Created download directory for version 'v1' of dataset: 'full_scale' at: /content/ego4d_data/v1/full_scale\n",
            "Only downloading a subset of the video files because the 'video_uids' flag has been set on the command line or in the config file. A total of 36 video files will be downloaded.\n",
            "\n",
            "Retrieving object metadata from S3...\n",
            "100% 36/36 [00:00<00:00, 1316.69object/s]\n",
            "Checking if latest file versions are already downloaded...\n",
            "100% 36/36 [00:05<00:00,  6.27file/s]\n",
            "No existing videos to filter.\n",
            "Downloading 36 files..\n",
            "100% 26.3G/26.3G [05:52<00:00, 144MiB/s]Checking file integrity...\n",
            "100% 26.3G/26.3G [05:52<00:00, 80.1MiB/s]\n"
          ]
        }
      ],
      "source": [
        "!ego4d \\\n",
        "  --output_directory /content/ego4d_data \\\n",
        "  --datasets full_scale \\\n",
        "  --version v1 \\\n",
        "  --video_uid_file video_uid_list.txt \\\n",
        "  -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uChlYwjmySqM",
        "outputId": "14b17349-a181-4c7c-d7e6-0ea468825dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted: /content/ego4d_data/v1/clips_top50/805989f6-0696-4de2-ad9b-0f194e0ac48d_clip_00.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/f681f510-cd33-48e3-bc10-4a8f2a518495_clip_01.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/fb7cc35d-3272-44a4-b8f2-15cd24fa345b_clip_02.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/7f4225ed-a076-4530-91cf-f3903c5d7637_clip_03.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/603ecf93-d2c1-4611-a690-76afd18935c8_clip_04.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/86343e9e-b932-41d3-ad6f-83f2c2fe5486_clip_05.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/b737cd68-4e0d-440a-9813-a6c90080fac5_clip_06.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/8b9b9816-d6eb-4544-818e-9d59e400b80d_clip_07.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/224c3de4-9683-462a-8eb4-224773425a7e_clip_08.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/b884e44c-07d8-426e-8356-1be4905c2675_clip_09.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/fb7cc35d-3272-44a4-b8f2-15cd24fa345b_clip_10.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/3534864b-2289-4aaf-b3ed-10eeeee7acd2_clip_11.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/18a3840b-7463-43c4-9aa9-b1d8e486fa84_clip_12.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/4480f4ee-d218-41bd-8cdf-a7cbe13b61a0_clip_13.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/4ce119de-0f42-4bd1-b387-9e19643fdddc_clip_14.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/1938c632-f575-49dd-8ae0-e48dbb467920_clip_15.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/2c5a603d-b9e8-45b2-8ff7-f079f6f402fc_clip_16.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/180b72fa-2102-414e-a6d5-2806e53f320c_clip_17.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/ae2dfec4-6094-4ff3-a404-a8a6ab1e0f02_clip_18.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/12946f28-5f4a-4ba5-8296-c08cbeeca501_clip_19.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/c864505c-9d49-4da4-bf7e-254b4c348c03_clip_20.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/4480f4ee-d218-41bd-8cdf-a7cbe13b61a0_clip_21.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/2876b375-e848-412c-8a6f-0664cbab6a33_clip_22.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/7f70ae0a-d097-4dda-9afa-318188dfce2d_clip_23.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/66a0900b-069d-48bf-9071-f4e659f8a9d0_clip_24.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/056db3f1-f957-46c8-b16b-c8fce22e78f9_clip_25.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/1938c632-f575-49dd-8ae0-e48dbb467920_clip_26.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/245cde61-d0a0-49c8-840d-e484b5f71a04_clip_27.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/a4728450-c05c-4d51-91e4-348f571d74bc_clip_28.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/30b9ff64-51c8-4481-ba6a-733ea2060aef_clip_29.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/50b2fa2f-9d39-4b9c-8e3c-4a4ef0669956_clip_30.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/8b9b9816-d6eb-4544-818e-9d59e400b80d_clip_31.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/12946f28-5f4a-4ba5-8296-c08cbeeca501_clip_32.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/42c08776-8832-4d37-8844-3ce89e405c62_clip_33.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/0c6ab491-c45e-4cbc-b6b2-a709cc856a9c_clip_34.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/224c3de4-9683-462a-8eb4-224773425a7e_clip_35.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/413fe086-1745-4573-b75b-e7d26ff72df9_clip_36.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/50b2fa2f-9d39-4b9c-8e3c-4a4ef0669956_clip_37.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/7f70ae0a-d097-4dda-9afa-318188dfce2d_clip_38.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/2d442f12-3595-4b2d-b8c5-583b65c0c49f_clip_39.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/30b9ff64-51c8-4481-ba6a-733ea2060aef_clip_40.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/1938c632-f575-49dd-8ae0-e48dbb467920_clip_41.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/ff6d3d52-dda5-46dd-8515-b9b772933030_clip_42.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/530dd2a8-462c-450b-9f38-1dfff28f139e_clip_43.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/50b2fa2f-9d39-4b9c-8e3c-4a4ef0669956_clip_44.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/ff6d3d52-dda5-46dd-8515-b9b772933030_clip_45.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/d88d4e54-5e5e-4c08-9a87-80958f9e4ec1_clip_46.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/224c3de4-9683-462a-8eb4-224773425a7e_clip_47.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/c864505c-9d49-4da4-bf7e-254b4c348c03_clip_48.mp4\n",
            "Extracted: /content/ego4d_data/v1/clips_top50/a850112c-68e3-45d7-9fc0-9ada21e970a4_clip_49.mp4\n",
            "Extraction completed.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "python /content/NLQ/EXTENSION2/extract_clips.py \\\n",
        "    --queries_file \"/content/NLQ/EXTENSION2/top50_annotated.json\" \\\n",
        "    --video_dir \"/content/ego4d_data/v1/full_scale\" \\\n",
        "    --clips_dir \"/content/ego4d_data/v1/clips_top50\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gofb9STbKHSy"
      },
      "outputs": [],
      "source": [
        "# To save space you can remove full scale videos as you don't need them anymore\n",
        "!rm -rf /content/ego4d_data/v1/full_scale"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Correct Answers"
      ],
      "metadata": {
        "id": "3d-bQbLBgV1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"What tool did I use on the machine first?\"**\n",
        "*   **GT Answer**: \"The tool you used first on the machine was a screwdriver.\"\n",
        "*   **LLaVA answer**: \"The tool you used first on the machine was a screwdriver.\"\n",
        "\n",
        "Scores: Bleu: 1.0, Rouge-L: 1.0, Meteor: 0.9997"
      ],
      "metadata": {
        "id": "dqeeG27y3bek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video(\"/content/ego4d_data/v1/clips_top50/f681f510-cd33-48e3-bc10-4a8f2a518495_clip_01.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "r0qglrX43adJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"How much soap did I apply on the sponge?\"**\n",
        "*   **GT Answer**: \"You applied a drop of soap on the sponge.\"\n",
        "*   **LLaVA answer**: \"You applied a drop of soap on the sponge.\"\n",
        "\n",
        "Scores: Bleu: 0.3586,\n",
        "    Rouge-L: 0.6957,\n",
        "    Meteor: 0.8042"
      ],
      "metadata": {
        "id": "VPT8-MUb32KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/224c3de4-9683-462a-8eb4-224773425a7e_clip_47.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "ffPEbw6F5C32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"Where is the red bicycle?\"**\n",
        "*   **GT Answer**: \"The red bicycle was parked near the entrance of a building.\"\n",
        "*   **LLaVA answer**: \"The red bicycle is parked on the sidewalk in front of a building. (or a store).\"\n",
        "\n",
        "Scores: Bleu: 0.1846,\n",
        "    Rouge-L: 0.5926,\n",
        "    Meteor: 0.5377"
      ],
      "metadata": {
        "id": "5ViQzQ6T4d3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/2876b375-e848-412c-8a6f-0664cbab6a33_clip_22.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "YmKPSdv45tKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Wrong Answers"
      ],
      "metadata": {
        "id": "q7YwWcoM0Di2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precise Counting"
      ],
      "metadata": {
        "id": "lOehpKuH0HXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"How many drawers did I open?\"**\n",
        "*   **GT Answer**: \"You opened six drawers.\"\n",
        "*   **LLaVA answer**: \"You opened two drawers.\"\n",
        "\n",
        "Scores: Bleu: 0,\n",
        "   Rouge-L: 0.4286,\n",
        "    Meteor: 0.535"
      ],
      "metadata": {
        "id": "ggz_9Ott0OLg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmLKqQw7gCUY"
      },
      "outputs": [],
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/805989f6-0696-4de2-ad9b-0f194e0ac48d_clip_00.mp4\", embed=True, width=640, height=480)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"How many doughs did i apply egg on?\"**\n",
        "*   **GT Answer**: \"You applied egg on ten doughs.\"\n",
        "*   **LLaVA answer**: \"I applied egg on 12 doughs. (croissants).\"\n",
        "\n",
        "Scores:  Bleu: 0,\n",
        "    Rouge-L: 0.6154,\n",
        "    Meteor: 0.6027"
      ],
      "metadata": {
        "id": "UehcYLAC1KSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/3534864b-2289-4aaf-b3ed-10eeeee7acd2_clip_11.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "HPlHBQEt1a-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis**: The model's quantitative reasoning is severely lacking and its ability to count sequential actions or multiple objects is not reliable. This suggests a fundamental weakness in visual reasoning that requires enumeration."
      ],
      "metadata": {
        "id": "ow-BasdC73pV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Timing"
      ],
      "metadata": {
        "id": "b_KDrO366iUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"what time did i open the fridge?\"**\n",
        "*   **GT Answer**: \"You opened the fridge at second 2.\"\n",
        "*   **LLaVA answer**: \"I opened the fridge at 11:30. (11:30 AM).\"\n",
        "\n",
        "Scores: Bleu: 0.1619,\n",
        "    Rouge-L: 0.4,\n",
        "    Meteor: 0.3351"
      ],
      "metadata": {
        "id": "z-30zW446nnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/fb7cc35d-3272-44a4-b8f2-15cd24fa345b_clip_10.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "UZV3Ci-g64fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"What time did i operate the machine?\"**\n",
        "*   **GT Answer**: \"You operated the machine at second 1.\"\n",
        "*   **LLaVA answer**: \"The machine was operated at 10:30. (10:30:00).\"\n",
        "\n",
        "Scores: Bleu: 0,\n",
        "    Rouge-L: 0.3529,\n",
        "    Meteor: 0.4482"
      ],
      "metadata": {
        "id": "GVpxVC857GoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/c864505c-9d49-4da4-bf7e-254b4c348c03_clip_20.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "2rcc2GO77TIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis**: This is a profound misconception. The GT refers to a timestamp within the video's timeline. LLaVA, however, invents a realistic time of day (\"11:30 AM\" for example). This indicates that the model relies on linguistic priors (questions starting with \"what time\" typically expect a time-of-day answer) over visual evidence, leading to a complete breakdown in understanding.\n"
      ],
      "metadata": {
        "id": "OvalY8UJ7s0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visual Ambiguity"
      ],
      "metadata": {
        "id": "h0uNzoEu9z8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"what did I put in the fry pan?\"**\n",
        "*   **GT Answer**: \"You put oil in the fry pan.\"\n",
        "*   **LLaVA answer**: \"You put a spoon in the fry pan.\"\n",
        "\n",
        "Scores: Bleu: 0.5133,\n",
        "    Rouge-L: 0.8,\n",
        "    Meteor: 0.8541"
      ],
      "metadata": {
        "id": "JxORE_NO93Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/4480f4ee-d218-41bd-8cdf-a7cbe13b61a0_clip_21.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "P0SPTzjd-GaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"Where was the plate before I picked it up?\"**\n",
        "*   **GT Answer**: \"The plate was in a drawer before you took it.\"\n",
        "*   **LLaVA answer**: \"The plate was in the refrigerator before you picked it up. (from the refrigerator shelf)\"\n",
        "\n",
        "Scores: Bleu: 0.379,\n",
        "    Rouge-L: 0.6923,\n",
        "    Meteor: 0.7622"
      ],
      "metadata": {
        "id": "L-fT5Oq9-vBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/50b2fa2f-9d39-4b9c-8e3c-4a4ef0669956_clip_44.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "xn2Kp81_-6jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query: \"What vegetable did I cut?\"**\n",
        "*   **GT Answer**: \"You cut a cabbage.\"\n",
        "*   **LLaVA answer**: \"You cut a cucumber.\"\n",
        "\n",
        "Scores: Bleu: 0,\n",
        "    Rouge-L: 0.75,\n",
        "    Meteor: 0.75"
      ],
      "metadata": {
        "id": "Ww5OQzJFBZrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Video(\"/content/ego4d_data/v1/clips_top50/056db3f1-f957-46c8-b16b-c8fce22e78f9_clip_25.mp4\", embed=True, width=640, height=480)"
      ],
      "metadata": {
        "id": "FpchJ8jpAgmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis**: LLaVA generally captures the actions and overall context in egocentric videos but struggles with object and action grounding in visually ambiguous or similar situations. When objects resemble others (e.g., a shelf looking like a fridge or a cabbage looking like a cucumber) or actions involve subtle elements (e.g., pouring oil), the model tends to rely on plausibility biases rather than precise visual evidence. This highlights a key limitation: while NLQ can locate relevant video segments, extracting exact textual answers remains challenging in complex, unstructured scenes."
      ],
      "metadata": {
        "id": "C5Fs096Q_jvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General Analysis of Errors in Object and Action Grounding\n",
        "In this type of task, LLaVA generally succeeds in recognizing the performed action and the broad context of a query but often struggles with fine-grained grounding of objects and subtle actions. This is particularly evident in egocentric videos, where:\n",
        "Objects may appear visually similar (e.g., cucumber vs cabbage) or resemble other elements in the scene (e.g., a shelf resembling a fridge).\n",
        "Subtle or transparent objects and implicit actions (e.g., oil in a pan, small kitchen items) are difficult to detect reliably.\n",
        "In such cases, the model tends to rely on dataset biases or general plausibility, rather than precise visual evidence, leading to misidentification of objects or details of actions.\n",
        "Overall, these errors highlight a key limitation in this task: while the NLQ step efficiently identifies the relevant video interval, Video-LLaVA may fail to extract exact textual answers when objects are ambiguous, actions are subtle, or fine visual details are crucial. Addressing these challenges would require improved fine-grained visual grounding and object/action disambiguation in egocentric settings."
      ],
      "metadata": {
        "id": "qhsBYNLIBRzC"
      }
    }
  ]
}